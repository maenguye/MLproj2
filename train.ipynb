{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "import mpimg\n",
    "import matplotlib.image as mpimg  # Ensure correct import\n",
    "from NN import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/maelynenguyen/Desktop/79ca5c85-0d00-45fd-81e9-aa787898ebf5_epfml-segmentation/training/images/'\n",
    "path = os.getcwd() + '/dataset/training/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_001.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_002.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_003.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_004.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_005.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_006.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_007.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_008.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_009.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_010.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_011.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_012.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_013.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_014.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_015.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_016.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_017.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_018.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_019.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_020.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_021.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_022.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_023.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_024.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_025.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_026.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_027.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_028.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_029.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_030.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_031.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_032.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_033.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_034.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_035.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_036.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_037.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_038.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_039.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_040.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_041.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_042.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_043.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_044.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_045.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_046.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_047.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_048.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_049.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_050.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_051.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_052.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_053.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_054.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_055.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_056.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_057.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_058.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_059.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_060.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_061.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_062.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_063.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_064.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_065.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_066.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_067.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_068.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_069.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_070.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_071.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_072.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_073.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_074.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_075.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_076.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_077.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_078.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_079.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_080.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_081.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_082.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_083.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_084.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_085.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_086.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_087.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_088.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_089.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_090.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_091.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_092.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_093.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_094.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_095.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_096.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_097.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_098.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_099.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_100.png\n"
     ]
    }
   ],
   "source": [
    "data = extract_data(path, 100, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_001.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_002.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_003.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_004.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_005.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_006.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_007.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_008.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_009.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_010.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_011.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_012.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_013.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_014.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_015.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_016.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_017.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_018.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_019.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_020.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_021.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_022.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_023.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_024.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_025.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_026.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_027.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_028.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_029.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_030.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_031.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_032.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_033.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_034.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_035.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_036.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_037.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_038.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_039.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_040.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_041.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_042.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_043.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_044.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_045.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_046.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_047.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_048.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_049.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_050.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_051.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_052.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_053.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_054.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_055.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_056.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_057.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_058.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_059.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_060.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_061.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_062.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_063.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_064.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_065.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_066.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_067.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_068.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_069.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_070.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_071.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_072.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_073.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_074.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_075.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_076.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_077.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_078.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_079.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_080.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_081.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_082.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_083.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_084.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_085.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_086.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_087.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_088.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_089.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_090.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_091.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_092.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_093.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_094.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_095.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_096.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_097.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_098.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_099.png\n",
      "Loading /Users/melina/Desktop/ML/MLproj2/dataset/training/images/satImage_100.png\n"
     ]
    }
   ],
   "source": [
    "labels = extract_labels(path,100,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = int(0.1 * len(data))\n",
    "test_size = int(0.1 * len(data))\n",
    "\n",
    "train_data = data[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "val_data = data[train_size:train_size+val_size]\n",
    "val_labels = labels[train_size:train_size+val_size]\n",
    "test_data = data[train_size+val_size:]\n",
    "test_labels = labels[train_size+val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points per class: c0 = 15831, c1 = 34169\n",
      "Balancing training data...\n",
      "Number of data points per class after balancing: c0 = 15831, c1 = 15831\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "\n",
    "# Dataset and balancing\n",
    "c0, c1 = 0, 0\n",
    "for label in train_labels:\n",
    "    if label[0] == 1:\n",
    "        c0 += 1\n",
    "    else:\n",
    "        c1 += 1\n",
    "print(f\"Number of data points per class: c0 = {c0}, c1 = {c1}\")\n",
    "\n",
    "print(\"Balancing training data...\")\n",
    "min_c = min(c0, c1)\n",
    "idx0 = [i for i, label in enumerate(train_labels) if label[0] == 1]\n",
    "idx1 = [i for i, label in enumerate(train_labels) if label[1] == 1]\n",
    "new_indices = idx0[:min_c] + idx1[:min_c]\n",
    "\n",
    "train_data = train_data[new_indices]\n",
    "train_labels = train_labels[new_indices]\n",
    "\n",
    "c0, c1 = 0, 0\n",
    "for label in train_labels:\n",
    "    if label[0] == 1:\n",
    "        c0 += 1\n",
    "    else:\n",
    "        c1 += 1\n",
    "print(f\"Number of data points per class after balancing: c0 = {c0}, c1 = {c1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_channels, img_patch_size, num_labels):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear((img_patch_size // 4) * (img_patch_size // 4) * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, num_labels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        # x = x.reshape(x.size(0), -1)  # Use reshape instead of view\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "SEED = 42\n",
    "NUM_CHANNELS = 3  # Example, adjust as needed\n",
    "IMG_PATCH_SIZE = 16\n",
    "NUM_LABELS = 2\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data), torch.tensor(train_labels))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31662, 16, 16, 3), (31662, 2))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.14349827622086977\n",
      "Epoch 2/10, Loss: 0.07340081940113151\n",
      "Epoch 3/10, Loss: 0.05299746775335275\n",
      "Epoch 4/10, Loss: 0.05045303524230169\n",
      "Epoch 5/10, Loss: 0.04179121173817266\n",
      "Epoch 6/10, Loss: 0.04371056631360059\n",
      "Epoch 7/10, Loss: 0.036924062809267716\n",
      "Epoch 8/10, Loss: 0.038400296942808096\n",
      "Epoch 9/10, Loss: 0.0364379816507176\n",
      "Epoch 10/10, Loss: 0.030370681660979146\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Model, loss, and optimizer\n",
    "torch.manual_seed(SEED)\n",
    "model = SimpleCNN(NUM_CHANNELS, IMG_PATCH_SIZE, NUM_LABELS)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_data), torch.tensor(test_labels))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6250, 16, 16, 3), (6250, 2))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXEL_DEPTH = 255\n",
    "\n",
    "# Write predictions from neural network to a file\n",
    "def write_predictions_to_file(predictions, labels, filename):\n",
    "    max_labels = np.argmax(labels, 1)\n",
    "    max_predictions = np.argmax(predictions, 1)\n",
    "    file = open(filename, \"w\")\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(0, n):\n",
    "        file.write(max_labels(i) + \" \" + max_predictions(i))\n",
    "    file.close()\n",
    "\n",
    "\n",
    "# Print predictions from neural network\n",
    "def print_predictions(predictions, labels):\n",
    "    max_labels = np.argmax(labels, 1)\n",
    "    max_predictions = np.argmax(predictions, 1)\n",
    "    print(str(max_labels) + \" \" + str(max_predictions))\n",
    "\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            if labels[idx][0] > 0.5:  # bgrd\n",
    "                l = 0\n",
    "            else:\n",
    "                l = 1\n",
    "            array_labels[j : j + w, i : i + h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * PIXEL_DEPTH).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    n_channels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if n_channels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)\n",
    "        gt_img_3c[:, :, 0] = gt_img8\n",
    "        gt_img_3c[:, :, 1] = gt_img8\n",
    "        gt_img_3c[:, :, 2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "    color_mask[:, :, 0] = predicted_img * PIXEL_DEPTH\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, \"RGB\").convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, \"RGB\").convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming `model` is your trained PyTorch model\n",
    "\n",
    "def get_prediction(img):\n",
    "    # Convert the input image to a NumPy array (if not already in this format)\n",
    "    data = np.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n",
    "    \n",
    "    # Convert NumPy array to PyTorch tensor, add batch dimension, and normalize if needed\n",
    "    # data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "    \n",
    "    # Ensure the tensor is on the same device as the model (GPU/CPU)\n",
    "\n",
    "    # data_loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # data_tensor = data_loader.to(device)\n",
    "    \n",
    "    # Get the model's output (no need to apply softmax manually, PyTorch's CrossEntropyLoss handles this internally)\n",
    "    output = model(data)\n",
    "    \n",
    "    # Apply softmax for prediction probabilities (optional, depending on your needs)\n",
    "    softmax_output = torch.softmax(output, dim=1)\n",
    "    \n",
    "    # Get the prediction (index of the highest probability class)\n",
    "    output_prediction = softmax_output.argmax(dim=1).cpu().numpy()  # Move to CPU and convert to numpy\n",
    "    \n",
    "    # Convert the prediction to an image (adjust according to your label_to_img function)\n",
    "    img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "    \n",
    "    return img_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_with_groundtruth(filename, image_idx):\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    \n",
    "    # Read the image\n",
    "    img = plt.imread(image_filename)  # Use plt.imread for image loading\n",
    "    \n",
    "    # Get the prediction\n",
    "    img_prediction = get_prediction(img)\n",
    "    \n",
    "    # Concatenate prediction with ground truth\n",
    "    cimg = concatenate_images(img, img_prediction)  # Assuming concatenate_images works with the same input\n",
    "    \n",
    "    return cimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_with_overlay(filename, image_idx):\n",
    "    imageid = \"satImage_%.3d\" % image_idx\n",
    "    image_filename = filename + imageid + \".png\"\n",
    "    \n",
    "    # Read the image\n",
    "    img = plt.imread(image_filename)\n",
    "    \n",
    "    # Get the prediction\n",
    "    img_prediction = get_prediction(img)\n",
    "    \n",
    "    # Create the overlay (you may need to define make_img_overlay if it doesn't exist)\n",
    "    oimg = make_img_overlay(img, img_prediction)\n",
    "    \n",
    "    return oimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(prediction_training_dir)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, TRAINING_SIZE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     pimg \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction_with_groundtruth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     Image\u001b[38;5;241m.\u001b[39mfromarray(pimg)\u001b[38;5;241m.\u001b[39msave(\n\u001b[1;32m     27\u001b[0m         prediction_training_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m     oimg \u001b[38;5;241m=\u001b[39m get_prediction_with_overlay(train_data_filename, i)\n",
      "Cell \u001b[0;32mIn[88], line 9\u001b[0m, in \u001b[0;36mget_prediction_with_groundtruth\u001b[0;34m(filename, image_idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimread(image_filename)  \u001b[38;5;66;03m# Use plt.imread for image loading\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the prediction\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m img_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Concatenate prediction with ground truth\u001b[39;00m\n\u001b[1;32m     12\u001b[0m cimg \u001b[38;5;241m=\u001b[39m concatenate_images(img, img_prediction)  \u001b[38;5;66;03m# Assuming concatenate_images works with the same input\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[104], line 17\u001b[0m, in \u001b[0;36mget_prediction\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert NumPy array to PyTorch tensor, add batch dimension, and normalize if needed\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# data_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get the model's output (no need to apply softmax manually, PyTorch's CrossEntropyLoss handles this internally)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Apply softmax for prediction probabilities (optional, depending on your needs)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m softmax_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/base-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/base-venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 13\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Temporarily permute dimensions inside the model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables for evaluation metrics\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "\n",
    "# Disable gradient calculation during testing\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "        \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels.argmax(dim=1))\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        prediction_training_dir = \"predictions_training/\"\n",
    "        TRAINING_SIZE = len(test_loader)\n",
    "        train_data_filename = \"dataset/training/images/\"\n",
    "        \n",
    "        if not os.path.isdir(prediction_training_dir):\n",
    "            os.mkdir(prediction_training_dir)\n",
    "        for i in range(1, TRAINING_SIZE + 1):\n",
    "            pimg = get_prediction_with_groundtruth(train_data_filename, i)\n",
    "            Image.fromarray(pimg).save(\n",
    "                prediction_training_dir + \"prediction_\" + str(i) + \".png\"\n",
    "            )\n",
    "            oimg = get_prediction_with_overlay(train_data_filename, i)\n",
    "            oimg.save(prediction_training_dir + \"overlay_\" + str(i) + \".png\")\n",
    "        # Get predicted labels\n",
    "        # _, predicted = torch.max(outputs, 1)  # Choose the class with the highest output value\n",
    "        \n",
    "        # # Update correct predictions count\n",
    "        # total += labels.size(0)\n",
    "        # correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy and average loss\n",
    "# accuracy = 100 * correct / total\n",
    "average_loss = running_loss / len(test_loader)\n",
    "\n",
    "# print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Test Loss: {average_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLCourse1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
